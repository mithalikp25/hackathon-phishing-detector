"""
Apply feature extraction to your cleaned phishing dataset
This script processes your cleaned CSV and creates a feature-rich dataset
"""

import pandas as pd
import numpy as np
from feature_extraction import URLFeatureExtractor
import os

def main():
    # ==========================================
    # STEP 1: Load your cleaned dataset
    # ==========================================
    print("="*60)
    print("PHISHING URL FEATURE EXTRACTION PIPELINE")
    print("="*60)
    
    # Update this path to your cleaned CSV file
    input_file = "phishing_dataset_clean.csv"  
    output_file = "phishing_features.csv"
    
    print(f"\n[1/5] Loading dataset from: {input_file}")
    
    if not os.path.exists(input_file):
        print(f"ERROR: File '{input_file}' not found!")
        print("Please update the 'input_file' variable with your cleaned CSV filename.")
        return
    
    # Load the data
    df = pd.read_csv(input_file)
    print(f"✓ Loaded {len(df)} URLs")
    print(f"  Columns: {list(df.columns)}")
    
    # Rename 'text' column to 'url' if needed
    if 'text' in df.columns and 'url' not in df.columns:
        df = df.rename(columns={'text': 'url'})
        print(f"✓ Renamed 'text' column to 'url'")
    
    # Choose your dataset size
    # Options:
    #   None = Full dataset (782k URLs, ~90 mins)
    #   100000 = 100k URLs (~20 mins, excellent accuracy)
    #   50000 = 50k URLs (~12 mins, very good accuracy)
    #   10000 = 10k URLs (~3 mins, good for testing)
    
    SAMPLE_SIZE = 100000  # Perfect for MVP! Change this if needed
    
    if SAMPLE_SIZE and len(df) > SAMPLE_SIZE:
        print(f"\n⚠️  SAMPLE MODE: Processing {SAMPLE_SIZE:,} URLs")
        print(f"   Full dataset has {len(df):,} URLs")
        df = df.head(SAMPLE_SIZE)
    else:
        print(f"\n✅ FULL DATASET MODE: Processing all {len(df):,} URLs")
        print(f"   Estimated time: ~{len(df) // 10000 * 0.5:.0f}-{len(df) // 10000 * 0.7:.0f} minutes")
        print(f"   This is a one-time process - grab a coffee! ☕")
    
    # Check if 'url' and 'label' columns exist
    if 'url' not in df.columns:
        print("\nERROR: 'url' column not found in dataset!")
        print(f"Available columns: {list(df.columns)}")
        print("Please ensure your CSV has a column named 'url'")
        return
    
    if 'label' not in df.columns:
        print("\nWARNING: 'label' column not found!")
        print("If your dataset has labels with a different name, please rename it to 'label'")
    
    # ==========================================
    # STEP 2: Display dataset statistics
    # ==========================================
    print(f"\n[2/5] Dataset Statistics:")
    print(f"  Total URLs: {len(df)}")
    
    if 'label' in df.columns:
        label_counts = df['label'].value_counts()
        print(f"  Label distribution:")
        for label, count in label_counts.items():
            percentage = (count / len(df)) * 100
            label_name = "Phishing" if label == 1 else "Legitimate"
            print(f"    {label_name} ({label}): {count} ({percentage:.1f}%)")
        
        # Check if dataset is balanced
        balance_ratio = min(label_counts) / max(label_counts)
        if balance_ratio < 0.8:
            print(f"\n  ⚠️  Dataset is imbalanced (ratio: {balance_ratio:.2f})")
            print(f"      Consider balancing or using stratified sampling")
        else:
            print(f"\n  ✓ Dataset is well-balanced (ratio: {balance_ratio:.2f})")
    
    # ==========================================
    # STEP 3: Extract features
    # ==========================================
    print(f"\n[3/5] Extracting features from URLs...")
    print("  This may take a few minutes depending on dataset size...")
    
    extractor = URLFeatureExtractor()
    features_df = extractor.extract_features_from_dataframe(df, url_column='url')
    
    print(f"\n✓ Feature extraction complete!")
    print(f"  Total features extracted: {len(features_df.columns) - 2}")  # Exclude 'url' and 'label'
    
    # ==========================================
    # STEP 4: Display feature summary
    # ==========================================
    print(f"\n[4/5] Feature Summary:")
    
    # Get numeric columns only (exclude url and label)
    numeric_cols = features_df.select_dtypes(include=[np.number]).columns
    numeric_cols = [col for col in numeric_cols if col != 'label']
    
    print(f"\n  Sample features (first 5 rows):")
    display_cols = ['url', 'url_length', 'is_https', 'has_ip', 
                    'suspicious_keyword_count', 'entropy', 'label']
    display_cols = [col for col in display_cols if col in features_df.columns]
    print(features_df[display_cols].head())
    
    # Show feature statistics
    print(f"\n  Feature statistics:")
    stats_cols = ['url_length', 'dot_count', 'suspicious_keyword_count', 
                  'subdomain_count', 'entropy']
    stats_cols = [col for col in stats_cols if col in features_df.columns]
    
    if stats_cols:
        print(features_df[stats_cols].describe())
    
    # Check for missing values
    missing_values = features_df.isnull().sum()
    if missing_values.sum() > 0:
        print(f"\n  ⚠️  Missing values detected:")
        missing_cols = missing_values[missing_cols > 0]
        for col, count in missing_cols.items():
            print(f"    {col}: {count} missing values")
        
        print(f"\n  Filling missing values with 0...")
        features_df = features_df.fillna(0)
    else:
        print(f"\n  ✓ No missing values found")
    
    # ==========================================
    # STEP 5: Save the feature dataset
    # ==========================================
    print(f"\n[5/5] Saving feature dataset...")
    
    features_df.to_csv(output_file, index=False)
    print(f"✓ Saved to: {output_file}")
    print(f"  Shape: {features_df.shape}")
    print(f"  Size: {os.path.getsize(output_file) / (1024*1024):.2f} MB")
    
    # ==========================================
    # Final Summary
    # ==========================================
    print("\n" + "="*60)
    print("FEATURE EXTRACTION COMPLETE!")
    print("="*60)
    print(f"\n✓ Your feature-rich dataset is ready: {output_file}")
    print(f"✓ Total features: {len(features_df.columns) - 2}")
    print(f"✓ Total samples: {len(features_df)}")
    print(f"\nNext Steps:")
    print(f"  1. Review the features in {output_file}")
    print(f"  2. Proceed to Phase 3: Train your ML model")
    print(f"  3. Run: python train_model.py")
    print("="*60)


if __name__ == "__main__":
    main()




# ============================================================
# PHISHING URL FEATURE EXTRACTION PIPELINE
# ============================================================

# [1/5] Loading dataset from: phishing_dataset_clean.csv
# ✓ Loaded 782382 URLs
#   Columns: ['text', 'label']
# ✓ Renamed 'text' column to 'url'

# ⚠️  SAMPLE MODE: Processing 100,000 URLs
#    Full dataset has 782,382 URLs

# [2/5] Dataset Statistics:
#   Total URLs: 100000
#   Label distribution:
#     Legitimate (0): 53580 (53.6%)
#     Phishing (1): 46420 (46.4%)

#   ✓ Dataset is well-balanced (ratio: 0.87)

# [3/5] Extracting features from URLs...
#   This may take a few minutes depending on dataset size...
# Extracting features from 100000 URLs...
# Processed 0/100000 URLs...
# Processed 1000/100000 URLs...
# Processed 2000/100000 URLs...
# Processed 3000/100000 URLs...
# Processed 4000/100000 URLs...
# Processed 5000/100000 URLs...
# Processed 6000/100000 URLs...
# Processed 7000/100000 URLs...
# Processed 8000/100000 URLs...
# Processed 9000/100000 URLs...
# Processed 10000/100000 URLs...
# Processed 11000/100000 URLs...
# Processed 12000/100000 URLs...
# Processed 13000/100000 URLs...
# Processed 14000/100000 URLs...
# Processed 15000/100000 URLs...
# Processed 16000/100000 URLs...
# Processed 17000/100000 URLs...
# Processed 18000/100000 URLs...
# Processed 19000/100000 URLs...
# Processed 20000/100000 URLs...
# Processed 21000/100000 URLs...
# Processed 22000/100000 URLs...
# Processed 23000/100000 URLs...
# Processed 24000/100000 URLs...
# Processed 25000/100000 URLs...
# Processed 26000/100000 URLs...
# Processed 27000/100000 URLs...
# Processed 28000/100000 URLs...
# Processed 29000/100000 URLs...
# Processed 30000/100000 URLs...
# Processed 31000/100000 URLs...
# Processed 32000/100000 URLs...
# Processed 33000/100000 URLs...
# Processed 34000/100000 URLs...
# Processed 35000/100000 URLs...
# Processed 36000/100000 URLs...
# Processed 37000/100000 URLs...
# Processed 38000/100000 URLs...
# Processed 39000/100000 URLs...
# Processed 40000/100000 URLs...
# Processed 41000/100000 URLs...
# Processed 42000/100000 URLs...
# Processed 43000/100000 URLs...
# Processed 44000/100000 URLs...
# Processed 45000/100000 URLs...
# Processed 46000/100000 URLs...
# Processed 47000/100000 URLs...
# Processed 48000/100000 URLs...
# Processed 49000/100000 URLs...
# Processed 50000/100000 URLs...
# Processed 51000/100000 URLs...
# Processed 52000/100000 URLs...
# Processed 53000/100000 URLs...
# Processed 54000/100000 URLs...
# Processed 55000/100000 URLs...
# Processed 56000/100000 URLs...
# Processed 57000/100000 URLs...
# Processed 58000/100000 URLs...
# Processed 59000/100000 URLs...
# Processed 60000/100000 URLs...
# Processed 61000/100000 URLs...
# Processed 62000/100000 URLs...
# Processed 63000/100000 URLs...
# Processed 64000/100000 URLs...
# Processed 65000/100000 URLs...
# Processed 66000/100000 URLs...
# Processed 67000/100000 URLs...
# Processed 68000/100000 URLs...
# Processed 69000/100000 URLs...
# Processed 70000/100000 URLs...
# Processed 71000/100000 URLs...
# Processed 72000/100000 URLs...
# Processed 73000/100000 URLs...
# Processed 74000/100000 URLs...
# Processed 75000/100000 URLs...
# Processed 76000/100000 URLs...
# Processed 77000/100000 URLs...
# Processed 78000/100000 URLs...
# Processed 79000/100000 URLs...
# Processed 80000/100000 URLs...
# Processed 81000/100000 URLs...
# Processed 82000/100000 URLs...
# Processed 83000/100000 URLs...
# Processed 84000/100000 URLs...
# Processed 85000/100000 URLs...
# Processed 86000/100000 URLs...
# Processed 87000/100000 URLs...
# Processed 88000/100000 URLs...
# Processed 89000/100000 URLs...
# Processed 90000/100000 URLs...
# Processed 91000/100000 URLs...
# Processed 92000/100000 URLs...
# Processed 93000/100000 URLs...
# Processed 94000/100000 URLs...
# Processed 95000/100000 URLs...
# Processed 96000/100000 URLs...
# Processed 97000/100000 URLs...
# Processed 98000/100000 URLs...
# Processed 99000/100000 URLs...
# Feature extraction complete! Shape: (100000, 45)

# ✓ Feature extraction complete!
#   Total features extracted: 43

# [4/5] Feature Summary:

#   Sample features (first 5 rows):
#                                                  url  url_length  is_https  has_ip  suspicious_keyword_count   entropy  label
# 0      http://webmail-brinkster.com/ex/?email=%20%0%          45         0       0                         0  4.491419      1
# 1                         billsportsmaps.com/?p=1206          26         0       0                         0  4.103910      0
# 2  www.sanelyurdu.com/language/homebank.tsbbank.c...          61         0       0                         0  4.343688      1
# 3                          ee-billing.limited323.com          25         0       0                         0  3.703465      1
# 4                   indiadaily.com/bolly_archive.htm          32         0       0                         0  4.015320      0

#   Feature statistics:
#           url_length      dot_count  suspicious_keyword_count  subdomain_count        entropy
# count  100000.000000  100000.000000             100000.000000    100000.000000  100000.000000
# mean       48.906720       2.185840                  0.105570         0.662150       4.127390
# std        42.958865       1.543246                  0.400608         1.094806       0.395611
# min        15.000000       0.000000                  0.000000         0.000000       2.289246
# 25%        28.000000       1.000000                  0.000000         0.000000       3.879664
# 50%        38.000000       2.000000                  0.000000         0.000000       4.110577
# 75%        56.000000       3.000000                  0.000000         1.000000       4.373495
# max      1492.000000      36.000000                  7.000000        26.000000       7.427747

#   ✓ No missing values found

# [5/5] Saving feature dataset...
# ✓ Saved to: phishing_features.csv
#   Shape: (100000, 45)
#   Size: 20.09 MB

# ============================================================
# FEATURE EXTRACTION COMPLETE!
# ============================================================

# ✓ feature-rich dataset is ready: phishing_features.csv
# ✓ Total features: 43
# ✓ Total samples: 100000
